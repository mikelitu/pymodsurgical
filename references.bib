
@inproceedings{allard_sofa_2007,
	address = {Palm Beach, United States.},
	title = {{SOFA} - an {Open} {Source} {Framework} for {Medical} {Simulation}},
	abstract = {SOFA is a new open source framework primarily targeted at medical simulation research. Based on an advanced software architecture, it allows to (1) create complex and evolving simulations by combining new algorithms with algorithms already included in SOFA; (2) modify most parameters of the simulation – deformable behavior, surface representation, solver, constraints, collision algorithm, etc. – by simply editing an XML ﬁle; (3) build complex models from simpler ones using a scene-graph description; (4) efﬁciently simulate the dynamics of interacting objects using abstract equation solvers; and (5) reuse and easily compare a variety of available methods. In this paper we highlight the key concepts of the SOFA architecture and illustrate its potential through a series of examples.},
	language = {en},
	booktitle = {{MMVR} 15 - {Medicine} {Meets} {Virtual} {Reality}},
	author = {Allard, Jérémie and Cotin, Stéphane and Faure, François and Bensoussan, Pierre-Jean and Poyer, François and Duriez, Christian and Delingette, Hervé and Grisoni, Laurent},
	month = feb,
	year = {2007},
	pages = {13--18},
	file = {Allard et al. - SOFA - an Open Source Framework for Medical Simula.pdf:/home/md21/Zotero/storage/PUPZP4CN/Allard et al. - SOFA - an Open Source Framework for Medical Simula.pdf:application/pdf},
}

@article{hauser_interactive_2003,
	title = {Interactive {Deformation} {Using} {Modal} {Analysis} with {Constraints}},
	abstract = {Modal analysis provides a powerful tool for efﬁciently simulating the behavior of deformable objects. This paper shows how manipulation, collision, and other constraints may be implemented easily within a modal framework. Results are presented for several example simulations. These results demonstrate that for many applications the errors introduced by linearization are acceptable, and that the resulting simulations are fast and stable even for complex objects and stiff materials.},
	language = {en},
	journal = {Graphics Interface},
	author = {Hauser, Kris K and Shen, Chen and O'Brien, James},
	year = {2003},
	file = {Hauser et al. - 2003 - Interactive Deformation Using Modal Analysis with .pdf:/home/md21/Zotero/storage/JNKKESME/Hauser et al. - 2003 - Interactive Deformation Using Modal Analysis with .pdf:application/pdf},
}

@article{li_space-time_2014,
	title = {Space-time editing of elastic motion through material optimization and reduction},
	volume = {33},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/2601097.2601217},
	doi = {10.1145/2601097.2601217},
	abstract = {We present a novel method for elastic animation editing with space-time constraints. In a sharp departure from previous approaches, we not only optimize control forces added to a linearized dynamic model, but also optimize material properties to better match user constraints and provide plausible and consistent motion. Our approach achieves efficiency and scalability by performing all computations in a reduced rotation-strain (RS) space constructed with both cubature and geometric reduction, leading to two orders of magnitude improvement over the original RS method. We demonstrate the utility and versatility of our method in various applications, including motion editing, pose interpolation, and estimation of material parameters from existing animation sequences.},
	number = {4},
	urldate = {2023-11-29},
	journal = {ACM Transactions on Graphics},
	author = {Li, Siwang and Huang, Jin and de Goes, Fernando and Jin, Xiaogang and Bao, Hujun and Desbrun, Mathieu},
	month = jul,
	year = {2014},
	keywords = {elastic animation, model reduction, motion editing, space and time constraints},
	pages = {108:1--108:10},
	file = {Full Text PDF:/home/md21/Zotero/storage/3RSL5HQ6/Li et al. - 2014 - Space-time editing of elastic motion through mater.pdf:application/pdf},
}

@article{huang_interactive_2011,
	title = {Interactive {Shape} {Interpolation} through {Controllable} {Dynamic} {Deformation}},
	volume = {17},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/5557868},
	doi = {10.1109/TVCG.2010.109},
	abstract = {In this paper, we introduce an interactive approach to generate physically based shape interpolation between poses. We extend linear modal analysis to offer an efficient and robust numerical technique to generate physically-plausible dynamics even for very large deformation. Our method also provides a rich set of intuitive editing tools with real-time feedback, including control over vibration frequencies, amplitudes, and damping of the resulting interpolation sequence. We demonstrate the versatility of our approach through a series of complex dynamic shape interpolations.},
	number = {7},
	urldate = {2023-11-29},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Huang, Jin and Tong, Yiying and Zhou, Kun and Bao, Hujun and Desbrun, Mathieu},
	month = jul,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {983--992},
	file = {IEEE Xplore Abstract Record:/home/md21/Zotero/storage/SH46TGL5/5557868.html:text/html;IEEE Xplore Full Text PDF:/home/md21/Zotero/storage/54VXLGZT/Huang et al. - 2011 - Interactive Shape Interpolation through Controllab.pdf:application/pdf},
}

@book{reddy_junthula_introduction_2013,
	title = {An introduction to the finite element method},
	volume = {3},
	publisher = {New York: McGraw-Hill},
	author = {Reddy, Junthula},
	year = {2013},
}

@article{james_dyrt_2002,
	title = {{DyRT}: {Dynamic} {Response} {Textures} for {Real} {Time} {Deformation} {Simulation} with {Graphics} {Hardware}},
	abstract = {In this paper we describe how to simulate geometrically complex, interactive, physically-based, volumetric, dynamic deformation models with negligible main CPU costs. This is achieved using a Dynamic Response Texture, or DyRT, that can be mapped onto any conventional animation as an optional rendering stage using commodity graphics hardware. The DyRT simulation process employs precomputed modal vibration models excited by rigid body motions. We present several examples, with an emphasis on bone-based character animation for interactive applications.},
	language = {en},
	year = {2002},
	month           = jul,
	volume		  = 21,
	number		  = 3,
	pages		  = {582--585},
	journal = {ACM Transactions on Graphics},
	author = {James, Doug L and Pai, Dinesh K},
	file = {James and Pai - DyRT Dynamic Response Textures for Real Time Defo.pdf:/home/md21/Zotero/storage/MPNZ9TBN/James and Pai - DyRT Dynamic Response Textures for Real Time Defo.pdf:application/pdf},
}

@misc{ye_self-supervised_2017,
	title = {Self-{Supervised} {Siamese} {Learning} on {Stereo} {Image} {Pairs} for {Depth} {Estimation} in {Robotic} {Surgery}},
	url = {http://arxiv.org/abs/1705.08260},
	doi = {10.48550/arXiv.1705.08260},
	abstract = {Robotic surgery has become a powerful tool for performing minimally invasive procedures, providing advantages in dexterity, precision, and 3D vision, over traditional surgery. One popular robotic system is the da Vinci surgical platform, which allows preoperative information to be incorporated into live procedures using Augmented Reality (AR). Scene depth estimation is a prerequisite for AR, as accurate registration requires 3D correspondences between preoperative and intraoperative organ models. In the past decade, there has been much progress on depth estimation for surgical scenes, such as using monocular or binocular laparoscopes [1,2]. More recently, advances in deep learning have enabled depth estimation via Convolutional Neural Networks (CNNs) [3], but training requires a large image dataset with ground truth depths. Inspired by [4], we propose a deep learning framework for surgical scene depth estimation using self-supervision for scalable data acquisition. Our framework consists of an autoencoder for depth prediction, and a differentiable spatial transformer for training the autoencoder on stereo image pairs without ground truth depths. Validation was conducted on stereo videos collected in robotic partial nephrectomy.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Ye, Menglong and Johns, Edward and Handa, Ankur and Zhang, Lin and Pratt, Philip and Yang, Guang-Zhong},
	month = may,
	year = {2017},
	note = {arXiv:1705.08260 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/6B8UXMCU/Ye et al. - 2017 - Self-Supervised Siamese Learning on Stereo Image P.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/52NT7CKF/1705.html:text/html},
}

@article{giannarou_probabilistic_2013,
	title = {Probabilistic tracking of affine-invariant anisotropic regions},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.81},
	abstract = {Despite a wide range of feature detectors developed in the computer vision community over the years, direct application of these techniques to surgical navigation has shown significant difficulties due to the paucity of reliable salient features coupled with free--form tissue deformation and changing visual appearance of surgical scenes. The aim of this paper is to propose a novel probabilistic framework to track affine-invariant anisotropic regions under contrastingly different visual appearances during Minimally Invasive Surgery (MIS). The theoretical background of the affine-invariant anisotropic feature detector is presented and a real-time implementation exploiting the computational power of the GPU is proposed. An Extended Kalman Filter (EKF) parameterization scheme is used to adaptively adjust the optimal templates of the detected regions, enabling accurate identification and matching of the tracked features. For effective tracking verification, spatial context and region similarity have also been incorporated. They are used to boost the prediction of the EKF and recover potential tracking failure due to drift or false positives. The proposed framework is compared to the existing methods and their respective performance is evaluated with in vivo video sequences recorded from robotic-assisted MIS procedures, as well as real-world scenes.},
	language = {eng},
	number = {1},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Giannarou, Stamatia and Visentini-Scarzanella, Marco and Yang, Guang-Zhong},
	month = jan,
	year = {2013},
	pmid = {22450819},
	keywords = {Algorithms, Surgery, Computer-Assisted, Anisotropy, Artificial Intelligence, Decision Support Techniques, Image Interpretation, Computer-Assisted, Pattern Recognition, Automated, Subtraction Technique},
	pages = {130--143},
}

@misc{kumar_generative_2023,
	title = {Generative {Image} {Dynamics} {Summary}},
	url = {https://prasantdixit.medium.com/generative-image-dynamics-summary-18668ab92259},
	abstract = {Recently a new Google Research has been published under the name of Generative Image Dynamics which seems astonishing to everyone because…},
	language = {en},
	urldate = {2023-11-15},
	journal = {Medium},
	author = {Kumar, Prasant},
	month = sep,
	year = {2023},
	file = {Snapshot:/home/md21/Zotero/storage/X24Z53MW/generative-image-dynamics-summary-18668ab92259.html:text/html},
}

@misc{plainswipe_what_2023,
	title = {What is frequency co-ordinated diffusion sampling?},
	url = {https://medium.com/@rk4082262341/what-is-frequency-co-ordinated-diffusion-sampling-d00e0e563280},
	abstract = {Explained with code},
	language = {en},
	urldate = {2023-11-15},
	journal = {Medium},
	author = {PlainSwipe},
	month = sep,
	year = {2023},
	file = {Snapshot:/home/md21/Zotero/storage/YRN7CTQU/what-is-frequency-co-ordinated-diffusion-sampling-d00e0e563280.html:text/html},
}

@misc{kant_invs_2023,
	title = {{iNVS}: {Repurposing} {Diffusion} {Inpainters} for {Novel} {View} {Synthesis}},
	shorttitle = {{iNVS}},
	url = {http://arxiv.org/abs/2310.16167},
	doi = {10.48550/arXiv.2310.16167},
	abstract = {We present a method for generating consistent novel views from a single source image. Our approach focuses on maximizing the reuse of visible pixels from the source image. To achieve this, we use a monocular depth estimator that transfers visible pixels from the source view to the target view. Starting from a pre-trained 2D inpainting diffusion model, we train our method on the large-scale Objaverse dataset to learn 3D object priors. While training we use a novel masking mechanism based on epipolar lines to further improve the quality of our approach. This allows our framework to perform zero-shot novel view synthesis on a variety of objects. We evaluate the zero-shot abilities of our framework on three challenging datasets: Google Scanned Objects, Ray Traced Multiview, and Common Objects in 3D. See our webpage for more details: https://yashkant.github.io/invs/},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Kant, Yash and Siarohin, Aliaksandr and Vasilkovsky, Michael and Guler, Riza Alp and Ren, Jian and Tulyakov, Sergey and Gilitschenski, Igor},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16167 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/NH77MJ3Y/Kant et al. - 2023 - iNVS Repurposing Diffusion Inpainters for Novel V.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/8XD6NBQK/2310.html:text/html},
}

@misc{xing_dynamicrafter_2023,
	title = {{DynamiCrafter}: {Animating} {Open}-domain {Images} with {Video} {Diffusion} {Priors}},
	shorttitle = {{DynamiCrafter}},
	url = {http://arxiv.org/abs/2310.12190},
	doi = {10.48550/arXiv.2310.12190},
	abstract = {Enhancing a still image with motion offers more engaged visual experience. Traditional image animation techniques mainly focus on animating natural scenes with random dynamics, such as clouds and fluid, and thus limits their applicability to generic visual contents. To overcome this limitation, we explore the synthesis of dynamic content for open-domain images, converting them into animated videos. The key idea is to utilize the motion prior of text-to-video diffusion models by incorporating the image into the generative process as guidance. Given an image, we first project it into a text-aligned rich image embedding space using a learnable image encoding network, which facilitates the video model to digest the image content compatibly. However, some visual details still struggle to be preserved in the resulting videos. To supplement more precise image information, we further feed the full image to the diffusion model by concatenating it with the initial noises. Experimental results reveal that our proposed method produces visually convincing animated videos, exhibiting both natural motions and high fidelity to the input image. Comparative evaluation demonstrates the notable superiority of our approach over existing competitors. The source code will be released upon publication.},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Xing, Jinbo and Xia, Menghan and Zhang, Yong and Chen, Haoxin and Wang, Xintao and Wong, Tien-Tsin and Shan, Ying},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12190 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/BH7SF7K9/Xing et al. - 2023 - DynamiCrafter Animating Open-domain Images with V.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/JQZC5K2U/2310.html:text/html},
}

@misc{noauthor_diff-usionawesome-diffusion-models_nodate,
	title = {diff-usion/{Awesome}-{Diffusion}-{Models}: {A} collection of resources and papers on {Diffusion} {Models}},
	url = {https://github.com/diff-usion/Awesome-Diffusion-Models/tree/main#medical-imaging},
	urldate = {2023-10-31},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv:2105.05233 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/md21/Zotero/storage/DDHQUBQG/2105.html:text/html;Full Text PDF:/home/md21/Zotero/storage/S6NL7QAU/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/md21/Zotero/storage/A5STTVT3/2006.html:text/html;Full Text PDF:/home/md21/Zotero/storage/L3QYI49X/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@misc{zhou_magicvideo_2023,
	title = {{MagicVideo}: {Efficient} {Video} {Generation} {With} {Latent} {Diffusion} {Models}},
	shorttitle = {{MagicVideo}},
	url = {http://arxiv.org/abs/2211.11018},
	doi = {10.48550/arXiv.2211.11018},
	abstract = {We present an efficient text-to-video generation framework based on latent diffusion models, termed MagicVideo. MagicVideo can generate smooth video clips that are concordant with the given text descriptions. Due to a novel and efficient 3D U-Net design and modeling video distributions in a low-dimensional space, MagicVideo can synthesize video clips with 256x256 spatial resolution on a single GPU card, which takes around 64x fewer computations than the Video Diffusion Models (VDM) in terms of FLOPs. In specific, unlike existing works that directly train video models in the RGB space, we use a pre-trained VAE to map video clips into a low-dimensional latent space and learn the distribution of videos' latent codes via a diffusion model. Besides, we introduce two new designs to adapt the U-Net denoiser trained on image tasks to video data: a frame-wise lightweight adaptor for the image-to-video distribution adjustment and a directed temporal attention module to capture temporal dependencies across frames. Thus, we can exploit the informative weights of convolution operators from a text-to-image model for accelerating video training. To ameliorate the pixel dithering in the generated videos, we also propose a novel VideoVAE auto-encoder for better RGB reconstruction. We conduct extensive experiments and demonstrate that MagicVideo can generate high-quality video clips with either realistic or imaginary content. Refer to {\textbackslash}url\{https://magicvideo.github.io/\#\} for more examples.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Zhou, Daquan and Wang, Weimin and Yan, Hanshu and Lv, Weiwei and Zhu, Yizhe and Feng, Jiashi},
	month = may,
	year = {2023},
	note = {arXiv:2211.11018 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/44MNX2VL/Zhou et al. - 2023 - MagicVideo Efficient Video Generation With Latent.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/375UGG6I/2211.html:text/html},
}

@misc{zhao_thin-plate_2022,
	title = {Thin-{Plate} {Spline} {Motion} {Model} for {Image} {Animation}},
	url = {http://arxiv.org/abs/2203.14367},
	doi = {10.48550/arXiv.2203.14367},
	abstract = {Image animation brings life to the static object in the source image according to the driving video. Recent works attempt to perform motion transfer on arbitrary objects through unsupervised methods without using a priori knowledge. However, it remains a significant challenge for current unsupervised methods when there is a large pose gap between the objects in the source and driving images. In this paper, a new end-to-end unsupervised motion transfer framework is proposed to overcome such issue. Firstly, we propose thin-plate spline motion estimation to produce a more flexible optical flow, which warps the feature maps of the source image to the feature domain of the driving image. Secondly, in order to restore the missing regions more realistically, we leverage multi-resolution occlusion masks to achieve more effective feature fusion. Finally, additional auxiliary loss functions are designed to ensure that there is a clear division of labor in the network modules, encouraging the network to generate high-quality images. Our method can animate a variety of objects, including talking faces, human bodies, and pixel animations. Experiments demonstrate that our method performs better on most benchmarks than the state of the art with visible improvements in pose-related metrics.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Zhao, Jian and Zhang, Hui},
	month = mar,
	year = {2022},
	note = {arXiv:2203.14367 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/TQZVSHGL/Zhao and Zhang - 2022 - Thin-Plate Spline Motion Model for Image Animation.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/3DRABIUX/2203.html:text/html},
}

@article{davis_image-space_2015,
	title = {Image-space modal bases for plausible manipulation of objects in video},
	volume = {34},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2816795.2818095},
	doi = {10.1145/2816795.2818095},
	abstract = {We present algorithms for extracting an image-space representation of object structure from video and using it to synthesize physically plausible animations of objects responding to new, previously unseen forces. Our representation of structure is derived from an image-space analysis of modal object deformation: projections of an object’s resonant modes are recovered from the temporal spectra of optical ﬂow in a video, and used as a basis for the image-space simulation of object dynamics. We describe how to extract this basis from video, and show that it can be used to create physicallyplausible animations of objects without any knowledge of scene geometry or material properties.},
	language = {en},
	number = {6},
	urldate = {2023-10-31},
	journal = {ACM Transactions on Graphics},
	author = {Davis, Abe and Chen, Justin G. and Durand, Frédo},
	month = nov,
	year = {2015},
	pages = {1--7},
	file = {Davis et al. - 2015 - Image-space modal bases for plausible manipulation.pdf:/home/md21/Zotero/storage/4Z68IKIK/Davis et al. - 2015 - Image-space modal bases for plausible manipulation.pdf:application/pdf},
}

@article{chuang_animating_2005,
	title = {Animating {Pictures} with {Stochastic} {Motion} {Textures}},
	volume = {24},
	abstract = {In this paper, we explore the problem of taking a still picture and making it move in convincing ways. In this paper, we limit our domain to scenes containing passive elements that respond to natural forces in some oscillatory fashion. We use a semi-automatic approach, in which a human user segments the scene into a series of layers to be individually animated. The automatic part of the approach works by synthesizing a “stochastic motion texture” using a spectral method — i.e., a ﬁltered noise spectrum whose inverse Fourier transform is the motion texture. The motion texture is a time-varying 2D displacement map, which is applied to each layer. The resulting warped layers are recomposited, along with “inpainting” to ﬁll any holes, to form the animated frames. The result is a video texture created from a single still image, which has the advantages of being more controllable and of generally higher image quality and resolution than a video texture created from a video source. We demonstrate the technique on a variety of photographs and paintings.},
	language = {en},
	number = {3},
	journal = {ACM Transactions on Graphics},
	author = {Chuang, Yung-Yu and Goldman, Dan B and Curless, Brian and Salesin, David H and Szeliski, Richard},
	month = jul,
	year = {2005},
	pages = {853--860},
	file = {Chuang et al. - Animating Pictures with Stochastic Motion Textures.pdf:/home/md21/Zotero/storage/ICXM3NDD/Chuang et al. - Animating Pictures with Stochastic Motion Textures.pdf:application/pdf},
}

@inproceedings{dorkenwald_stochastic_2021,
	title = {Stochastic {Image}-to-{Video} {Synthesis} {Using} {cINNs}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Dorkenwald_Stochastic_Image-to-Video_Synthesis_Using_cINNs_CVPR_2021_paper.html?ref=https://githubhelp.com},
	language = {en},
	urldate = {2023-10-31},
	author = {Dorkenwald, Michael and Milbich, Timo and Blattmann, Andreas and Rombach, Robin and Derpanis, Konstantinos G. and Ommer, Bjorn},
	year = {2021},
	pages = {3742--3753},
	file = {Full Text PDF:/home/md21/Zotero/storage/TCW86XHP/Dorkenwald et al. - 2021 - Stochastic Image-to-Video Synthesis Using cINNs.pdf:application/pdf},
}

@misc{li_generative_2023,
	title = {Generative {Image} {Dynamics}},
	url = {http://arxiv.org/abs/2309.07906},
	doi = {10.48550/arXiv.2309.07906},
	abstract = {We present an approach to modeling an image-space prior on scene dynamics. Our prior is learned from a collection of motion trajectories extracted from real video sequences containing natural, oscillating motion such as trees, flowers, candles, and clothes blowing in the wind. Given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a per-pixel long-term motion representation in the Fourier domain, which we call a neural stochastic motion texture. This representation can be converted into dense motion trajectories that span an entire video. Along with an image-based rendering module, these trajectories can be used for a number of downstream applications, such as turning still images into seamlessly looping dynamic videos, or allowing users to realistically interact with objects in real pictures.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Li, Zhengqi and Tucker, Richard and Snavely, Noah and Holynski, Aleksander},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07906 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/QTTLAA5Y/Li et al. - 2023 - Generative Image Dynamics.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/4K795Y9M/2309.html:text/html},
}

@misc{endo_animating_2019,
	title = {Animating {Landscape}: {Self}-{Supervised} {Learning} of {Decoupled} {Motion} and {Appearance} for {Single}-{Image} {Video} {Synthesis}},
	shorttitle = {Animating {Landscape}},
	url = {http://arxiv.org/abs/1910.07192},
	doi = {10.48550/arXiv.1910.07192},
	abstract = {Automatic generation of a high-quality video from a single image remains a challenging task despite the recent advances in deep generative models. This paper proposes a method that can create a high-resolution, long-term animation using convolutional neural networks (CNNs) from a single landscape image where we mainly focus on skies and waters. Our key observation is that the motion (e.g., moving clouds) and appearance (e.g., time-varying colors in the sky) in natural scenes have different time scales. We thus learn them separately and predict them with decoupled control while handling future uncertainty in both predictions by introducing latent codes. Unlike previous methods that infer output frames directly, our CNNs predict spatially-smooth intermediate data, i.e., for motion, flow fields for warping, and for appearance, color transfer maps, via self-supervised learning, i.e., without explicitly-provided ground truth. These intermediate data are applied not to each previous output frame, but to the input image only once for each output frame. This design is crucial to alleviate error accumulation in long-term predictions, which is the essential problem in previous recurrent approaches. The output frames can be looped like cinemagraph, and also be controlled directly by specifying latent codes or indirectly via visual annotations. We demonstrate the effectiveness of our method through comparisons with the state-of-the-arts on video prediction as well as appearance manipulation.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Endo, Yuki and Kanamori, Yoshihiro and Kuriyama, Shigeru},
	month = oct,
	year = {2019},
	note = {arXiv:1910.07192 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/THJS83DA/Endo et al. - 2019 - Animating Landscape Self-Supervised Learning of D.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/VKWZH8BN/1910.html:text/html},
}

@inproceedings{hu_dynamic_2023,
	title = {A {Dynamic} {Multi}-{Scale} {Voxel} {Flow} {Network} for {Video} {Prediction}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Hu_A_Dynamic_Multi-Scale_Voxel_Flow_Network_for_Video_Prediction_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-01-25},
	author = {Hu, Xiaotao and Huang, Zhewei and Huang, Ailin and Xu, Jun and Zhou, Shuchang},
	year = {2023},
	pages = {6121--6131},
	file = {Full Text PDF:/home/md21/Zotero/storage/FLS45QNU/Hu et al. - 2023 - A Dynamic Multi-Scale Voxel Flow Network for Video.pdf:application/pdf},
}

@inproceedings{ni_conditional_2023,
	title = {Conditional {Image}-to-{Video} {Generation} {With} {Latent} {Flow} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-01-25},
	author = {Ni, Haomiao and Shi, Changhao and Li, Kai and Huang, Sharon X. and Min, Martin Renqiang},
	year = {2023},
	pages = {18444--18455},
	file = {Full Text PDF:/home/md21/Zotero/storage/N4QYJRCM/Ni et al. - 2023 - Conditional Image-to-Video Generation With Latent .pdf:application/pdf},
}

@misc{teed_raft_2020,
	title = {{RAFT}: {Recurrent} {All}-{Pairs} {Field} {Transforms} for {Optical} {Flow}},
	shorttitle = {{RAFT}},
	url = {http://arxiv.org/abs/2003.12039},
	doi = {10.48550/arXiv.2003.12039},
	abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Teed, Zachary and Deng, Jia},
	month = aug,
	year = {2020},
	note = {arXiv:2003.12039 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md21/Zotero/storage/4N229GCZ/Teed and Deng - 2020 - RAFT Recurrent All-Pairs Field Transforms for Opt.pdf:application/pdf;arXiv.org Snapshot:/home/md21/Zotero/storage/VER4VRFK/2003.html:text/html},
}

@article{petitjean_modalnerf_2023,
	title = {{ModalNeRF}: {Neural} {Modal} {Analysis} and {Synthesis} for {Free}-{Viewpoint} {Navigation} in {Dynamically} {Vibrating} {Scenes}},
	volume = {42},
	copyright = {© 2023 The Authors. Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {{ModalNeRF}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14888},
	doi = {10.1111/cgf.14888},
	abstract = {Recent advances in Neural Radiance Fields enable the capture of scenes with motion. However, editing the motion is hard; no existing method allows editing beyond the space of motion existing in the original video, nor editing based on physics. We present the first approach that allows physically-based editing of motion in a scene captured with a single hand-held video camera, containing vibrating or periodic motion. We first introduce a Lagrangian representation, representing motion as the displacement of particles, which is learned while training a radiance field. We use these particles to create a continuous representation of motion over the sequence, which is then used to perform a modal analysis of the motion thanks to a Fourier transform on the particle displacement over time. The resulting extracted modes allow motion synthesis, and easy editing of the motion, while inheriting the ability for free-viewpoint synthesis in the captured 3D scene from the radiance field. We demonstrate our new method on synthetic and real captured scenes.},
	language = {en},
	number = {4},
	urldate = {2024-02-15},
	journal = {Computer Graphics Forum},
	author = {Petitjean, Automne and Poirier-Ginter, Yohan and Tewari, Ayush and Cordonnier, Guillaume and Drettakis, George},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14888},
	pages = {e14888},
	file = {Full Text PDF:/home/md21/Zotero/storage/2TTMYMGI/Petitjean et al. - 2023 - ModalNeRF Neural Modal Analysis and Synthesis for.pdf:application/pdf;Snapshot:/home/md21/Zotero/storage/Q9TEZ5KC/cgf.html:text/html},
}

@article{scheikl_lapgym_2023,
	title = {{LapGym} - {An} {Open} {Source} {Framework} for {Reinforcement} {Learning} in {Robot}-{Assisted} {Laparoscopic} {Surgery}},
	volume = {24},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v24/23-0207.html},
	abstract = {Recent advances in reinforcement learning (RL) have increased the promise of introducing cognitive assistance and automation to robot-assisted laparoscopic surgery (RALS). However, progress in algorithms and methods depends on the availability of standardized learning environments that represent skills relevant to RALS. We present LapGym, a framework for building RL environments for RALS that models the challenges posed by surgical tasks, and sofaenv, a diverse suite of 12 environments. Motivated by surgical training, these environments are organized into 4 tracks: Spatial Reasoning, Deformable Object Manipulation \& Grasping, Dissection, and Thread Manipulation. Each environment is highly parametrizable for increasing difficulty, resulting in a high performance ceiling for new algorithms. We use Proximal Policy Optimization (PPO) to establish a baseline for model-free RL algorithms, investigating the effect of several environment parameters on task difficulty. Finally, we show that many environments and parameter configurations reflect well-known, open problems in RL research, allowing researchers to continue exploring these fundamental problems in a surgical context. We aim to provide a challenging, standard environment suite for further development of RL for RALS, ultimately helping to realize the full potential of cognitive surgical robotics. LapGym is publicly accessible through GitHub (https://github.com/ScheiklP/lap\_gym).},
	number = {368},
	urldate = {2024-02-16},
	journal = {Journal of Machine Learning Research},
	author = {Scheikl, Paul Maria and Gyenes, Balázs and Younis, Rayan and Haas, Christoph and Neumann, Gerhard and Wagner, Martin and Mathis-Ullrich, Franziska},
	year = {2023},
	pages = {1--42},
	file = {Full Text PDF:/home/md21/Zotero/storage/U2ZE7IQY/Scheikl et al. - 2023 - LapGym - An Open Source Framework for Reinforcemen.pdf:application/pdf;Source Code:/home/md21/Zotero/storage/8QQWJH5L/lap_gym.html:text/html},
}
